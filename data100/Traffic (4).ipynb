{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2eedd1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Traffic.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0c57c",
   "metadata": {},
   "source": [
    "# Final Project: Traffic\n",
    "## Due Date: Monday, December 13th, 11:59 PM\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with other groups about\n",
    "the project, we ask that you **write your solutions within your own groups**. If you do\n",
    "discuss the assignments with others outside of your group please **include their names** at the top\n",
    "of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba9096",
   "metadata": {},
   "source": [
    "# Data 100 Final Project: Traffic in a post-lockdown world\n",
    "\n",
    "**Scenario:** You're a data scientist at Uber -- sitting in a war room on March 16, 2020, 1 day after California-wide COVID lockdown measures began and the day shelter-in-place measures are announced in the bay area. The entire data science department is on fire: All of your existing traffic models have regressed *significantly*. Given the sudden change in traffic patterns (i.e., no traffic at all), the company's traffic estimates are wildly incorrect. This is a top priority for the company. Since traffic estimates are used directly for pricing strategies, this is actively costing the company millions every hour. You are tasked with fixing these models.\n",
    "\n",
    "**Takeaways:** How do you \"fix\" models that have learned biases from pre-lockdown traffic? How do you train new ones, with just 24 hours of data? What sorts of data do you examine, to better understand the situation? In the midst of company-wide panic, you'll need a strong inferential acumen to lead a robust data science response. In this project, we'll walk you through a simulated war room data science effort, culminating in some strategies to fix models online, which are experiencing large distributional shifts in data.\n",
    "\n",
    "For this project, we'll explore traffic data provided by the **Uber Movement** dataset, specifically around the start of COVID shutdowns in March 2020. Your project is structured around the following ideas:\n",
    "\n",
    "```\n",
    "1. Guided data cleaning: Clustering data spatially\n",
    "    a. Load Uber traffic speeds dataset\n",
    "    b. Map traffic speeds to Google Plus Codes (spatially uniform)\n",
    "        i. Load node-to-gps-coordinates data\n",
    "        ii. Map traffic speed to GPS coordinates\n",
    "        iii. Convert GPS coordinates to plus code regions\n",
    "        iv. Sanity check number of plus code regions in San Francisco\n",
    "        v. Plot a histogram of the standard deviation in speed, per plus code region.\n",
    "    c. Map traffic speeds to census tracts (spatially non-uniform)\n",
    "        i. Download census tracts geojson\n",
    "        ii. Map traffic speed to census tracts\n",
    "        iii. Sanity check number of census tracts in San Francisco with data.\n",
    "        iv. Plot a histogram of the standard deviation in speed, per census tract.\n",
    "    d. What defines a \"good\" or \"bad\" spatial clustering?\n",
    "2. Guided EDA: Understanding COVID lockdown impact on traffic\n",
    "    a. How did lockdown affect average traffic speeds?\n",
    "        i. Sort census tracts by average speed, pre-lockdown.\n",
    "        ii. Sort census tracts by average speed, post-lockdown.\n",
    "        iii. Sort census tracts by change in average speed, from pre to post lockdown.\n",
    "        iv. Quantify the impact of lockdown on average speeds.\n",
    "        v. Quantify the impact of pre-lockdown average speed on change in speed.\n",
    "    b. What traffic areas were impacted by lockdown?\n",
    "        i. Visualize heatmap of average traffic speed per census tract, pre-lockdown.\n",
    "        ii. Visualize change in average daily speeds pre vs. post lockdown.\n",
    "        iii. Quantify the impact of lockdown on daily speeds, spatially.\n",
    "3. Open-Ended EDA: Understanding lockdown impact on traffic times\n",
    "    a. Download Uber Movement (Travel Times) dataset\n",
    "4. Guided Modeling: Predict traffic speed post-lockdown\n",
    "    a. Predict daily traffic speed on pre-lockdown data\n",
    "        i. Assemble dataset to predict daily traffic speed.\n",
    "        ii. Train and evaluate linear model on pre-lockdown data.\n",
    "    b. Understand failures on post-lockdown data\n",
    "        i. Evaluate on post-lockdown data\n",
    "        ii. Report model performance temporally\n",
    "    c. \"Fix\" model on post-lockdown data\n",
    "        i. Learn delta off of a moving bias\n",
    "        ii. Does it \"solve itself\"? Does the pre-lockdown model predict, after the change point?\n",
    "        iii. Naively retrain model with post-lockdown data\n",
    "        iv. What if you just ignore the change point?\n",
    "5. Open-Ended Modeling: Predicting travel times post-lockdown\n",
    "```\n",
    "\n",
    "Concepts tested: regex, pivot, join, grouping, inferential thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac22504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import contextily as cx\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import Callable\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from zipfile import ZipFile\n",
    "zf = ZipFile('data.zip', 'r')\n",
    "zf.extractall('.')\n",
    "\n",
    "# more readable exceptions\n",
    "%pip install --quiet iwut\n",
    "%load_ext iwut\n",
    "%wut on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1170aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 1 - Guided Data Cleaning: Partitioning Data Spatially\n",
    "\n",
    "Our hope is answer: How do we group information spatially? We'll specifically look at 2 ways of partitioning data spatially, to understand the impact of spatial partitioning strategies on our analyses:\n",
    "\n",
    "1. Dividing the world uniformly into slices, like Google's plus codes.\n",
    "2. Dividing the world according to population, using census tracts.\n",
    "\n",
    "In this step, we'll load the following datasets that we'll need for this project:\n",
    "\n",
    "- Daily travel times from Uber Movement data in March 2020 from San Francisco, by census tract\n",
    "- Daily traffic speeds from Uber Movement data in Q1 2020 from San Francisco, between OSM nodes\n",
    "- Census tracts dividing San Francisco by GPS coordinates\n",
    "- Mapping from OSM nodes to GPS coordinates\n",
    "\n",
    "There are several terms and concepts to get familiar with upfront:\n",
    "\n",
    "- **Open Street Maps (OSM)** provides nodes (points in space, [wiki](https://wiki.openstreetmap.org/wiki/Node)) and ways (segments between nodes [wiki](https://wiki.openstreetmap.org/wiki/Way)). These IDs are used in the Uber Movement dataset to identify streets in the traffic speeds dataset.\n",
    "- **Census Tracts** provided by the county of San Francisco geographically divides space according to the US 2010 Census. This is used in the Uber Movement dataset to identify regions of differing travel times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972f63f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.a. Load Uber traffic speeds dataset\n",
    "\n",
    "The dataset is located at `data/movement-speeds-daily-san-francisco-2020-3.csv`. **Load this dataset into a dataframe.**\n",
    "\n",
    "*The original dataset from Uber was provided hourly and took up 2.1 GB on disk, which means it couldn't fit into your 1GB of RAM. You can find the dataset preparation script at `data/PrepareTrafficDataset.ipynb` which aggregated within each day, reducing the dataset to just 55MB on disk.*\n",
    "\n",
    "*This was originally going to be question in this project, but it takes 22 minutes to run. Better yet, if you mess up, your kernel dies and you start over. We deemed it too frustrating and preprocessed the dataset to spare you the pain... but just know that this is a real-world issue!*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Uber Movement (Movement Speeds) dataset into dataframe\n",
    "speeds_to_nodes = pd.read_csv('data/movement-speeds-daily-san-francisco-2020-3.csv')\n",
    "\n",
    "speeds_to_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f96bb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ce62e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.b. Map traffic speed to Google Plus Codes\n",
    "\n",
    "Google Plus Codes divide up the world uniformly into rectangular slices ([link](https://maps.google.com/pluscodes/)). Let's use this to segment traffic speeds spatially. Take a moment to answer: **Is this spatial structure effective for summarizing traffic speed?** Before completing this section, substantiate your answer with examples of your expectations (e.g., we expect A to be separated from B). After completing this section, substantiate your answer with observations you've made.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "points: 2\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfc8c4",
   "metadata": {},
   "source": [
    "Our expectation is that uniform rectangular slices are not the most effective spatial structure for summarizing traffic speeds, because it groups sections of geographical regions into uniform rectangles, regardless of the type of neighborhood / subregion that the rectangle includes. Commercial districts are mixed in with residential districts, as well as highways, parks, schools, and other features that heavily play into computing average traffic speeds. \n",
    "\n",
    "After completing the rest of the section, we find that expectations were correct, as supported by plus codes having a lower across-cluster standard deviation of within-cluster means, as compared to other spatial structures such as census tracts. This shows that, using pluscodes, the within-cluster means are relatively similar, so it's harder to tell which subregions are high speed / low speed, preventing us from making meaningful insights about the areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d25e2",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### 1.b.i. Load Node-to-GPS-Coordinate Data\n",
    "\n",
    "In this substep, we'll load a mapping from OSM nodes to GPS coordinates. The dataset is provided in a gzip'ed XML file from OpenStreetMaps (OSM). The mapping from OSM nodes to GPS coordinates was downloaded from https://download.bbbike.org/osm/bbbike/SanFrancisco/SanFrancisco.osm.gz. We've downloaded this for you, to avoid any issues with OSM updates.\n",
    "\n",
    "**If** you try to load the provided `.osm` (an `.xml` in disguise) using Python's built-in XML utilities **(by uncommenting the last 2 lines in the below cell)**, you will hit an out-of-memory error, as your kernel is forced to restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b734bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OSM] - Read the OSM XML and extract mapping from node ID to GPS coordinates\n",
    "PATH_OSM = os.path.expanduser('data/SanFrancisco.osm')\n",
    "\n",
    "# Runs out of memory! File itself is 430 MB, even when filtering out\n",
    "# irrelevant rows, and remaining 3M rows are too expensive to parse,\n",
    "# resulting in OOM\n",
    "\n",
    "# import xml.etree.ElementTree as ET\n",
    "# _tree = ET.parse(PATH_OSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de7d9e",
   "metadata": {},
   "source": [
    "Your above code hits a memory error, so instead, we will use our handy-dandy tool--regex--from earlier in the semester to load just the parts of the file that we need. **Given the XML snippet below, write a regex pattern to extract OSM node ID, latitude, and longitude.** (The first capture group should be node ID. The second should be latitude, and the third should be longitude.) A snippet of the XML is included below ([screenshot](https://extract.bbbike.org/extract-screenshots.html)):\n",
    "\n",
    "```\n",
    "<?xml version='1.0' encoding='UTF-8'?>\n",
    "<osm version=\"0.6\" generator=\"osmconvert 0.8.3\">\n",
    "    <bounds minlat=\"42.4543\" minlon=\"-2.4761999\" maxlat=\"42.4...\"/>\n",
    "    <node id=\"26861066\" lat=\"42.471111\" lon=\"-2.454722\" version=\"...\"/>\n",
    "        <tag k=\"name\" v=\"Camping La Playa\"/>\n",
    "        <tag k=\"tourism\" v=\"camp_site\"/>\n",
    "        <tag k=\"operator\" v=\"private\"/>\n",
    "        ...\n",
    "    </node>\n",
    "    <node id=\"34793287\" lat=\"42.4713587\" lon=\"-2.4510783\" version=\"...\"/>\n",
    "        <tag k=\"created_by\" v=\"JOSM\"/>\n",
    "    </node>\n",
    "    <node id=\"34793294\" lat=\"42.4610836\" lon=\"-2.4303622\" version=\"...\"/>\n",
    "    <node id=\"34793297\" lat=\"42.4548363\" lon=\"-2.4287657\" version=\"...\"/>\n",
    "    ...\n",
    "</osm>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82c675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [OSM] - Read the OSM XML using a regex operation instead.\n",
    "def read_node_lat_lon(path: str, pattern: str, line_condition: Callable):\n",
    "    \"\"\"\n",
    "    Read the provided path line at a line. If the provided regex pattern\n",
    "    has a match, return the grouped matches as items in a generator.\n",
    "    \n",
    "    :param path: Path to read data from\n",
    "    :param pattern: Regex pattern to test against each line\n",
    "    :param line_condition: function that returns if we should check regex\n",
    "        against current line\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            result = re.search(pattern, line)\n",
    "            if result is not None and line_condition(result):\n",
    "                yield int(result.group(1)), float(result.group(2)), float(result.group(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbae0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = set(speeds_to_nodes.osm_start_node_id) | set(speeds_to_nodes.osm_end_node_id)\n",
    "\n",
    "NODE_PATTERN = r\"\"\"id=.(\\d+)..lat=.(\\d+\\.\\d+)..lon=.(\\-?\\d+\\.\\d+)\"\"\"\n",
    "\n",
    "node_to_gps = pd.DataFrame(read_node_lat_lon(\n",
    "    PATH_OSM,\n",
    "    pattern=NODE_PATTERN,\n",
    "    line_condition=lambda result: int(result.group(1)) in node_ids\n",
    "), columns=['osm_node_id', 'Latitude', 'Longitude'])\n",
    "node_to_gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fe3a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1bi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2e3dd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.b.ii. Map traffic speed to GPS coordinates.\n",
    "\n",
    "Traffic speeds are currently connected to OSM nodes. You will then use the mapping from OSM nodes to GPS coordinates, to map traffic speeds to GPS coordinates. **Link each traffic speed measurement to the GPS coordinate of its starting node.**\n",
    "\n",
    "**Note**: For simplicity, assume each segment is associated with the node it *starts* with. \n",
    "\n",
    "**Hint**: Not all nodes are included in the OSM node mapping. Make sure to ignore any nodes without valid GPS coordinates.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1bii\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc5f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find mapping from traffic speeds to GPS coordinates\n",
    "speeds_to_gps = speeds_to_nodes.merge(node_to_gps, left_on=\"osm_start_node_id\", right_on= \"osm_node_id\", how= \"inner\")\n",
    "speeds_to_gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f931cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1bii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3be0e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.b.iii. Convert GPS coordinates to plus code regions.\n",
    "\n",
    "Plus code regions divide up the world into uniformly-sized rectangles, which we will assume is 0.012 degrees latitudiunally and longitudinally. **For each traffic speed row, compute the plus code region it belongs to**, based on its GPS coordinates.\n",
    "\n",
    "To do this, we suggest computing a latitudinal index `plus_latitude_idx` and a longitudinal index `plus_longitude_idx` for the plus code region each row belongs to. *Make sure these columns are integer-valued*.\n",
    "\n",
    "**Hint**: If you're running into nans, you did 1.b.ii. incorrectly!\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1biii\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # do this however you like\n",
    "speeds_to_gps['plus_latitude_idx'] = (speeds_to_gps['Latitude'] / 0.012).astype(int)\n",
    "speeds_to_gps['plus_longitude_idx'] = (speeds_to_gps['Longitude'] / 0.012).astype(int)\n",
    "speeds_to_gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43568f79",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1biii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6eccf8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.b.iv. Sanity check number of plus code regions in San Francisco.\n",
    "\n",
    "**Compute the number of unique plus codes found in your dataset**. You're checking that the number isn't ridiculous, like 1, or 100,000 (SF is 231 sq mi, so 100k tracts would average 12 sq ft per tract).\n",
    "\n",
    "If you followed the suggestion above, this is the number of unique `(plus_latitude_idx, plus_longitude_idx)` pairs.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1biv\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc353f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You're expecting 276 plus codes here. Don't just type \"276\" \n",
    "# below to pass the autograder. The goal is to sanity check your \n",
    "# dataframe!\n",
    "temp = speeds_to_gps[['plus_latitude_idx', 'plus_longitude_idx']]\n",
    "num_pluscode_regions = temp.drop_duplicates().shape[0]\n",
    "num_pluscode_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969c637",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1biv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d50542",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.b.v. How well do plus code regions summarize movement speeds?\n",
    "\n",
    "The following will give us an idea of how well the average represents traffic speed per plus code region. For these questions, we'll refer to a \"plus code region\" as a \"cluster\":\n",
    "\n",
    "1. **Plot a histogram of the within-cluster standard deviation**.\n",
    "2. **Compute across-cluster average of within-cluster standard deviation**.\n",
    "3. **Compute across-cluster standard deviation of within-cluster average speeds**.\n",
    "4. **Is this average variance reasonable?** To assess what \"reasonable\" means, consider these questions and how to answer them: (1) Do plus codes capture meaningful subpopulations? (2) Do differences between subpopulations outweigh differences within a subpopulation? Use the statistics above to answer these questions, and compute any additional statistics you need. Additionally explain *why these questions are important to assessing the quality of a spatial clustering*.\n",
    "\n",
    "**Hint**: Run the autograder first to ensure your variance average and average variance are correct, before starting to draw conclusions.\n",
    "\n",
    "In the first cell, write your written answers. In the second cell, complete the code.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1bv1\n",
    "points: 2\n",
    "manual: True\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f344ee2",
   "metadata": {},
   "source": [
    "For plus codes, the average variance is not a reasonable statistic. \n",
    "\n",
    "Plus codes do not capture meaningful subpopulations because they split a region up into uniform rectangles which can contain “slices” of many different traffic zones, such as highways (high speed), school zones (low speed), commercial & residential districts, etc. This causes within-cluster means to be less meaningful, because the mixture of these zones causes each cluster’s mean to be pretty similar. \n",
    "\n",
    "In theory, average variance represents how much the distribution of inner-cluster means is spread out. However, this statistic is inflated because the amount of data available for each cluster isn't consistent. For clusters with a lot of data points, we would expect the mean and standard deviations of traffic speeds to be accurate representations of real world traffic in that pluscode. However, statistically, clusters with less available data are more likely to have higher within-cluster standard deviations. \n",
    "\n",
    "Therefore, computing the across-cluster mean of within-cluster standard deviations gives disproportionate weight to clusters with less data, because each of the clusters hold the same weight when computing an average across all clusters. This inflates the average variance figure, thereby making it a less reasonable and reliable statistic. \n",
    "\n",
    "This is important in assessing the quality of spatial clustering because we need quantitative metrics to see if subpopulations carry any real meaning. For instance, a clustering method with a very high average variance tells us that when you look at any particular cluster, it is likely that the traffic speeds within that cluster vary heavily. Then, we can step back and assess if this clustering method has meaningful subpopulations, as we now see that we may have grouped too many high-speed and low-speed regions together in the same clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45a362",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1bv2\n",
    "points: 2\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9ec83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speed_variance_by_pluscode = speeds_to_gps.groupby(['plus_latitude_idx','plus_longitude_idx']).agg(np.std)\n",
    "average_variance_by_pluscode = np.mean(speed_variance_by_pluscode['speed_mph_mean'])\n",
    "variance_average_by_pluscode = speeds_to_gps.groupby(['plus_latitude_idx','plus_longitude_idx']).mean()[\"speed_mph_mean\"].std()\n",
    "plt.hist(speed_variance_by_pluscode['speed_mph_mean']);\n",
    "plt.ylabel(\"Number of Pluscodes\")\n",
    "plt.xlabel(\"Within-Cluster Standard Deviation of Speed in mph\")\n",
    "plt.title(\"Speed Variance by Pluscode\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b9177",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1bv3\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddeeddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_variance_by_pluscode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf561a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1bv3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a6e79",
   "metadata": {},
   "source": [
    "## 1.c. Map traffic speed to census tract.\n",
    "\n",
    "Census tracts divide the space much less uniformly, subdividing regions that we were interested in into smaller zones. This suggests promise in providing informative spatial segments. Note that the daily traffic speeds are provided between OpenStreetMap (OSM) nodes, so we'll need to map nodes to census tracts somehow.\n",
    "\n",
    "Above, we've mapped traffic speeds to GPS coordinates. Below, we'll then link GPS coordinates to census tracts, to complete the mapping from traffic speeds to census tracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02aca27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.c.i. Download Census Tracts Geojson\n",
    "\n",
    "**Load the census tracts geojson.** Make sure to see the relevant [geopandas io documentation](https://geopandas.org/docs/user_guide/io.html) to see how to load a geojson.\n",
    "\n",
    "**Hint**: It should take you just one line to load.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1ci\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c8c7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH_TRACTS = os.path.expanduser('data/san_francisco_censustracts.json')\n",
    "tract_to_gps = gpd.read_file(PATH_TRACTS)\n",
    "tract_to_gps['MOVEMENT_ID'] = tract_to_gps['MOVEMENT_ID'].astype(int)\n",
    "tract_to_gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad51eba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1ci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a801db2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.c.ii Map traffic speed to census tracts.\n",
    "\n",
    "You will need to *spatially join* the (1) mapping from traffic speed to GPS coordinates `speed_to_gps` and (2) the mapping from GPS coordinates to boundaries of census tracts `tract_to_gps` to group all traffic speeds by census tract. This \"spatial join\" is an advanced feature recently released (as of time of writing, in Oct 2021) in geopandas, which allows us to connect single points to their enclosing polygons. You will do this question in 3 parts:\n",
    "\n",
    "1. Convert the last dataframe `speeds_to_gps` into a geopandas dataframe `speeds_to_points`, where GPS coordinates are now geopandas points. See this tutorial: https://geopandas.org/gallery/create_geopandas_from_pandas.html#From-longitudes-and-latitudes\n",
    "2. Set the coordinate-system for the new geopandas dataframe to the \"world geodesic system\" [link](https://epsg.io/4326), or in other words, the coordinate system that GPS coordinates are reported in.\n",
    "3. Compute a spatial join between census tracts `tract_to_gps` and the geopandas traffic speeds `speeds_to_points`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1cii\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2640f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds_to_points = gpd.GeoDataFrame(speeds_to_gps, geometry=gpd.points_from_xy(speeds_to_gps.Longitude, speeds_to_gps.Latitude)).set_crs(4326)\n",
    "speeds_to_tract = tract_to_gps.sjoin(speeds_to_points, how = \"inner\").to_crs(4326)\n",
    "\n",
    "speeds_to_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8207f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1cii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1746a1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.c.iii. Aggregate movement speeds by census tract.\n",
    "\n",
    "- Create a new dataframe `speeds_by_tract` to group movement speeds by census tract. See the outputted dataframe from 1.c.i. to check how census tracts are identified.\n",
    "- Always double-check your numbers. **Report the number of census tracts** in your dataset.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1ciii\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds_by_tract = speeds_to_tract.groupby('MOVEMENT_ID')\n",
    "num_census_tracts = len(speeds_by_tract)\n",
    "num_census_tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea4bd8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1ciii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d28ba4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.c.iv. How well do census tracts summarize movement speeds?\n",
    "\n",
    "The following will give us an idea of how well the average represents traffic speed per plus code region. For these questions, we'll refer to a \"census tract\" as a \"cluster\":\n",
    "\n",
    "1. **Plot a histogram of the within-cluster standard deviation**.\n",
    "2. **Compute across-cluster average of within-cluster standard deviation**.\n",
    "3. **Compute across-cluster standard deviation of within-cluster average speeds**.\n",
    "4. **Is this average variance reasonable?** To assess what \"reasonable\" means, consider these questions and how to answer them: (1) Do plus codes capture meaningful subpopulations? (2) Do differences between subpopulations outweigh differences within a subpopulation? Use these ideas to assess whether the average standard deviation is high or not.\n",
    "\n",
    "Note: We are using the speed metric of miles per hour here.\n",
    "\n",
    "Just like before, please written answers in the first cell and coding answers in the second cell.\n",
    "<!--5. Using the above, how would you **compare census tracts to plus codes, in terms of its effectiveness** as a spatial clustering mechanism for analyzing traffic speeds? Compare the statistics you've computed. What does it mean for one to be higher than the other?-->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1civ1\n",
    "points: 2\n",
    "manual: True\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e477c2",
   "metadata": {},
   "source": [
    "This average variance is much more reasonable than the pluscode average variance. While the calculation for both ends up being similar (8.68 vs. 8.30), we see that average variance as a statistic is much more meaningful when used on census tracts.\n",
    "\n",
    "This is largely because census tracts are divided up to have roughly the same population in each tract, which reduces the problem we explained earlier, where clusters with low populations (number of data points) are disproportionately factored into the calculation of across-cluster average variance. In other words, because census tracts have more similar populations than pluscodes, we see that differences (deviation) within a cluster no longer outweigh the differences across all clusters, thereby making the average variance a much more reliable statistic. \n",
    "\n",
    "Similarly, census tracts summarize movement speeds better than plus code regions because the subregions are defined based on neighborhood type and population counts. For instance, specific suburbs are zoned together in a single census tract, seperated from commercial districts, industrial areas, etc. \n",
    "\n",
    "This makes each census tract more different from each other, which can be seen by the clusters' means varying more from one another. This is why we see an increase in across-cluster standard deviation of within-cluster average speeds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541ea2c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1civ2\n",
    "points: 2\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0659c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_variance_by_tract = speeds_to_tract.groupby(['MOVEMENT_ID']).agg(np.std)\n",
    "average_variance_by_tract = np.mean(speed_variance_by_tract['speed_mph_mean'])\n",
    "variance_average_by_tract = speeds_to_tract.groupby(['MOVEMENT_ID']).mean()[\"speed_mph_mean\"].std() \n",
    "plt.hist(speed_variance_by_tract['speed_mph_mean'])\n",
    "plt.ylabel(\"Number of Census Tracts\") \n",
    "plt.xlabel(\"Within-Cluster Standard Deviation of Speed in mph\") \n",
    "plt.title(\"Speed Variance by Tract\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4152b14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1civ3\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67893fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_variance_by_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450781dc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1civ3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8719bc4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.d. What would be the ideal spatial clustering?\n",
    "\n",
    "This is an active research problem in many spatiotemporal modeling communities, and there is no single agreed-upon answer. Answer both of the following specifically knowing that you'll need to analyze traffic patterns according to this spatial clustering:\n",
    "\n",
    "1. **What is a good metric for a spatial structure?** How do we define good? Bad? What information do we expect a spatial structure to yield? Use the above parts and questions to help answer this.\n",
    "2. **What would you do to optimize your own metric for success in a spatial structure?**\n",
    "\n",
    "See related articles:\n",
    "\n",
    "- Uber's H3 [link](https://eng.uber.com/h3/), which divides the world into hexagons\n",
    "- Traffic Analysis Zones (TAZ) [link](https://en.wikipedia.org/wiki/Traffic_analysis_zone), which takes census data and additionally accounts for vehicles per household when dividing space\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1d\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3deac",
   "metadata": {},
   "source": [
    "A good spatial structure is one where we can look at each cluster and generate area-specific insights, for instance looking at how traffic speeds change in different neighborhoods / subregions over time. In other words, when each cluster has a \"type\" (suburb, low-income or high income, commercial districts, downtown, highways, etc), we can make meaningful conclusions about how each \"type\" is affected by something like a lockdown. \n",
    "\n",
    "A bad spatial structure, in this case, is one where lines are drawn somewhat arbitrarily, grouping together different types of neighborhoods and districts in the same cluster. Lack of differentiation prevents meaningful insight across clusters. Also, a bad spatial cluster is one where the size of each cluster is too small or too big. If the clusters are too small, we don't generate much more insight than simply looking at each node. If the clusters are too large, entire districts / zones are grouped together, preventing us from making detailed insights. \n",
    "\n",
    "A good metric for spatial clustering would be a high variance of within-cluster means. If we see a high variance of within-cluster means, it indicates that the clusters represent different types of neighborhood / subregions, i.e. residential vs. industrial / highways. Another good metric would be across-cluster standard deviation of within-cluster number of data points. This would tell us if the amount of available data is similar across all clusters. We would likely want this variance to be low, meaning each cluster has roughly the same number of data points, allowing our calculations made across all clusters to be evenly-weighted.\n",
    "\n",
    "Addressing the more general question, we think the ideal spatial clustering for traffic speeds would be something like census tracts, where the entire region is separated by neighborhood type. Furthermore, if each cluster had the same amount of data available, statistics such as across-cluster standard deviation and mean would be more meaningful, because each cluster would be weighed equally, giving a more accurate representation instead of low-data clusters holding disproportionate weight. \n",
    "\n",
    "If we were creating our own spatial structure, we would want to “draw” and “redraw” our lines, adjusting in different ways to optimize for the metrics outlined above. For instance, if our across-cluster standard deviation of within-cluster means is too low, our subregions might be too similar to one another. In this case, we might want to split the region into more clusters, allowing each one to represent a more specific part of the city."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227a7d7",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Step 2 - Guided EDA: Understanding COVID Lockdown Impact on Traffic\n",
    "\n",
    "In this step, we'll examine the impact of COVID on traffic. In particular, we'll study 3 different questions:\n",
    "\n",
    "- How did lockdown affect traffic speed? What factors dictate how much lockdown affected traffic speed?\n",
    "- What areas of traffic were most impacted by lockdown?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6189c",
   "metadata": {},
   "source": [
    "## 2.a. How did lockdown affect traffic speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d34be",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.a.i. Sort census tracts by average speed, pre-lockdown.\n",
    "\n",
    "Consider the pre-lockdown period to be March 1 - 13, before the first COVID-related restrictions (travel bans) were announced on March 14, 2020.\n",
    "\n",
    "1. **Report a DataFrame which includes the *names* of the 10 census tracts with the lowest average speed**, along with the average speed for each tract.\n",
    "2. **Report a DataFrame which includes the *names* of the 10 census tracts with the highest average speed**, along with the average speed for each tract.\n",
    "2. Do these names match your expectations for low speed or high speed traffic pre-lockdown?  What relationships do you notice? (What do the low-speed areas have in common? The high-speed areas?) For this specific question, answer qualitatively. No need to quantify. **Hint**: Look up some of the names on a map, to understand where they are.\n",
    "3. **Plot a histogram for all average speeds, pre-lockdown**.\n",
    "4. You will notice a long tail distribution of high speed traffic. What do you think this corresponds to in San Francisco? Write down your hypothesis.\n",
    "\n",
    "Hint: To start off, think about what joins may be useful to get the desired DataFrame.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2ai1\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6ab04",
   "metadata": {},
   "source": [
    "The names of the lowest-speed census tracts definitely do match my expectation because they're all in high-congestion areas of San Francisco. As someone who grew up here, I expected neighborhoods like the Tenderloin, Mission District, and the Financial District to be very slow because a huge volume of cars and trucks occupy the streets throughout most of the day, slowing down traffic. Furthermore, many of these neighborhoods contain unusual roads and traffic conditions, such as trolleys, intersecting one-way streets, double bike lanes, looping freeway entrances, and other features that often cause confusion and slow down traffic due to uncertainty. \n",
    "\n",
    "The names of the high-speed census tracts also match my expectation because they're almost all near highways or in industrial parts of the Bay Area. For tracts that include highways, we would expect a much higher average traffic speed because highways have much higher speed limits. For tracts in industrial areas, such as Petrolite Street in Richmond, CA, we see many large open roads almost entirely occupied by working trucks and cars. It makes sense that these areas have high average traffic speeds because there is a lot of open space and not much residential / commercial activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3492fb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Answer the following question:\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2ai2\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1543e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute the average speed per census tract (will use this later),\n",
    "# BEFORE the shelter-in-place was announced on March 14, 2020.\n",
    "# Autograder expects this to be a series\n",
    "averages_pre = speeds_to_tract[speeds_to_tract['day'] < 14].groupby('MOVEMENT_ID').agg('mean').sort_values(by ='speed_mph_mean')['speed_mph_mean']\n",
    "# Autograder expects this to be a dataframe with name of census tract,\n",
    "# polygon for census tract, and average speed per census tract\n",
    "averages_pre_named = speeds_to_tract[['MOVEMENT_ID', 'DISPLAY_NAME','geometry']].drop_duplicates().merge(averages_pre.to_frame().reset_index())\n",
    "averages_pre_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b36e40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2ai2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8a88f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Report the lowest 10 census tracts with the lowest average speed\n",
    "Remember we want the NAME of each census tract too. For the autograder, please keep the name of the speed field, `speed_mph_mean`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2ai3\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0d961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bottom10_averages_pre = averages_pre_named.sort_values(by=['speed_mph_mean'])[:10]\n",
    "bottom10_averages_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31cb10",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2ai3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025b156",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Report the highest 10 census tracts with the highest average speed.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2ai4\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_averages_pre = averages_pre_named.sort_values(by=['speed_mph_mean'],ascending = False)[:10]\n",
    "top10_averages_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0890d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2ai4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236db138",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Plot the histogram\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2ai5\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c3925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(averages_pre)\n",
    "plt.xlabel(\"Average Speed Within Census Tract (mph)\")\n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Hisogram of Bay Area Census Tracts Average Speed Pre Lockdown\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7daae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.a.ii. Sort census tracts by average speed, post-lockdown.\n",
    "\n",
    "I suggest checking the top 10 and bottom 10 tracts by average speed, post-lockdown. Consider the post-lockdown period to be March 14 - 31, after the first COVID restrictions were established on March 14, 2020. It's a healthy sanity check. For this question, you should report:\n",
    "\n",
    "- **Plot a histogram for all average speeds, post-lockdown.**\n",
    "- **What are the major differences between this post-lockdown histogram relative to the pre-lockdown histogram above**? Anything surprising? What did you expect, and what did you find?\n",
    "\n",
    "Write the written answers in the cell below, and the coding answers in the cells after that.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2aii1\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21badcd1",
   "metadata": {},
   "source": [
    "The biggest difference between the histograms is that average speed went up significantly after the lockdown. Before the lockdown, the top 10 census tracts with the highest average speed ranged from (38.9 mph – 59.5 mph), averaging 46.4 mph across the 10 tracts. After the lockdown, the top 10 tracts ranged from (56.0 mph – 70.5 mph), averaging 64.9 mph across the 10 tracts.\n",
    "\n",
    "While the shape of the distributions is similar, the average speed in the highest tracts went way up after the lockdown. This is surprising to me, because I would have expected high speeds to be associated with being in a rush to get somewhere, but almost all schools, places of work, stores, etc. were shut down. However, I can also see how if fewer total cars were out on the roads, people's driving speeds aren't limited by the density of traffic around them, which could explain higher average speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267b65f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2aii2\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db352a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average speed per census tract (will use this later),\n",
    "# AFTER (and including) the first COVID restrictions were put into effect.\n",
    "# Autograder expects this to be a series\n",
    "averages_post = speeds_to_tract[speeds_to_tract['day'] >= 14].groupby('MOVEMENT_ID').agg('mean').sort_values(by ='speed_mph_mean')['speed_mph_mean']\n",
    "# Autograder expects this to be a dataframe with name of census tract,\n",
    "# polygon for census tract, and average speed per census tract\n",
    "averages_post_named = speeds_to_tract[['MOVEMENT_ID', 'DISPLAY_NAME','geometry']].drop_duplicates().merge(averages_post.to_frame().reset_index())\n",
    "averages_post_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae5025",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2aii2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d57ac7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Plot the histogram\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2aii3\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(averages_post)\n",
    "plt.xlabel(\"Average Speed Within Census Tract (mph)\")\n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Hisogram of Bay Area Census Tracts Average Speed Post Lockdown\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8debfe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.a.iii. Sort census tracts by change in traffic speed from pre to post lockdown.\n",
    "\n",
    "For each segment, compute the difference between the pre-lockdown average speed (March 1 - 13) and the post-lockdown average speed (March 14 - 31). **Plot a histogram of all differences.** Sanity check that the below histogram matches your observations of the histograms above, on your own.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2aiii\n",
    "points: 2\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autograder expects differences to be a series object with index\n",
    "# MOVEMENT_ID.\n",
    "diff = averages_pre_named.merge(averages_post_named, left_on = \"DISPLAY_NAME\",right_on = \"DISPLAY_NAME\", how = 'inner')\n",
    "diff['differences'] = diff['speed_mph_mean_y'] - diff['speed_mph_mean_x']\n",
    "differences = diff['differences']\n",
    "# plot the differences\n",
    "plt.hist(differences);\n",
    "plt.xlabel(\"Difference in Average Speed between Pre-Lockdown and Post-Lockdown\") \n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Differences in Average Speeds Before and After Lockdown\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33affd69",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2aiii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f64a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.a.iv. Quantify the impact of lockdown on average speeds.\n",
    "\n",
    "1. **Plot the average speed by day, across all segments**. Be careful not to plot the average of census tract averages instead. Recall the definition of segments from Q1.\n",
    "2. Is the change in speed smooth and gradually increasing? Or increasing sharply? Why? Use your real-world knowledge of announcements and measures during that time, in your explanation. You can use this list of bay area COVID-related dataes: https://abc7news.com/timeline-of-coronavirus-us-covid-19-bay-area-sf/6047519/\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2aiv1\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e89ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder expects this to be a series object containing the\n",
    "# data for your line plot -- average speeds per day.\n",
    "speeds_daily = speeds_to_tract.groupby('day').agg(np.mean)['speed_mph_mean'] \n",
    "plt.plot(speeds_daily)\n",
    "plt.xlabel(\"Day in March 2020\")\n",
    "plt.ylabel(\"Average Speed Across All Segments\")\n",
    "plt.title(\"Average Speed Across all Nodes in March 2020\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69038b87",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Write your written answer in the cell below\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2aiv2\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605563b",
   "metadata": {},
   "source": [
    "The change in speed is increasingly sharpy, specifically accelerating around March 17th. This is likely because on March 17, shelter in place went into effect in almost all Bay Area counties, notably San Francisco and Alameda counties being relevant to this dataset. \n",
    "\n",
    "Shelter in place caused most schools, places of work, stores, etc. to close down, meaning people didn't have anywhere to go. This likely caused decreases in traffic volume across the board, which would explain the sharp increase in average traffic speeds, because people's driving speeds were not limited by congestion and traffic density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad51b9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Ignore the empty cell below, just run the autograder to test the code above is correct.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2aiv3\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2783df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d8b3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2aiv3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ad0f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.a.v. Quantify the impact of pre-lockdown average speed on change in speed.\n",
    "\n",
    "1. Compute the correlation between change in speed and the *pre*-lockdown average speeds. Do we expect a positive or negative correlation, given our analysis above?\n",
    "2. Compute the correlation between change in speed and the post-lockdown average speeds.\n",
    "3. **How does the correlation in Q1 compare with the correlation in Q2?** You should expect a significant change in correlation value. What insight does this provide about traffic?\n",
    "\n",
    "Written answers in the first cell, coding answerts in the following cell.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2av1\n",
    "points: 2\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7933b",
   "metadata": {},
   "source": [
    "We would expect the correlation between change in speed and pre-lockdown average speeds to be positive, because it makes sense that roads with high speeds before the lockdown also increased in speed post-lockdown. As traffic speeds generally increased across the board, roads such as highways with already high “pre-lockdown speeds” likely freed up even more, shown by a positive “change in speed”. \n",
    "\n",
    "For a similar reason, we would expect the correlation between change in speed and post-lockdown average speeds to be positive, because roads with high “post-lockdown speeds” likely saw a large increase in speed as compared to pre-lockdown (positive change in speed). \n",
    "\n",
    "After computing both correlation coefficients, we see that both correlations are positive, as expected, but post-lockdown speed is much higher correlated with change in speed (0.79 vs. 0.46). This is interesting, as it tells us that, looking at all the roads post lockdown, slower streets did not increase very much, while higher-speed streets increased the most. \n",
    "\n",
    "This is likely because the roads with the highest capacity for speeds such as highways, saw the biggest increase in speeds. This makes sense because, pre-lockdown, highways might not necessarily have high average speeds, depending on how much traffic congestion they normally get. Then, when the lockdown decreased traffic congestion overall, these roads with high capacities for speed saw the biggest increase in speeds, as well as having a high post-lockdown average speed. \n",
    "\n",
    "In other words, the lockdown made it so traffic speeds better reflect speed limits. As such, high speed-limit roads, such as highways, saw the biggest increases in speeds because they have the capacity for high-speed traffic. Similarly, streets with low post-lockdown average speeds did not increase by much, likely because their low speeds weren’t due to traffic congestion, but instead by low speed limits, such as roads in school zones, steep hills, etc. \n",
    "\n",
    "All of this generates useful insights about real traffic phenomena, as we are putting together more detailed “profiles” for each road and how they are affected differently by something like a county-wide lockdown.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad929d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2av2\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pre_diff = np.corrcoef(diff['speed_mph_mean_x'], diff['differences'])[1][0]\n",
    "corr_post_diff = np.corrcoef(diff['speed_mph_mean_y'],diff['differences'])[1][0] \n",
    "corr_pre_diff, corr_post_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a894ce9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2av2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84bf0b",
   "metadata": {},
   "source": [
    "## 2.b. What traffic areas were impacted by lockdown?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c20b6a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.b.i. Visualize spatial heatmap of average traffic speed per census tract, pre-lockdown.\n",
    "\n",
    "Visualize a spatial heatmap of the grouped average daily speeds per census tract, which you computed in previous parts. Use the geopandas [chloropleth maps](https://geopandas.org/docs/user_guide/mapping.html#choropleth-maps). **Write your observations, using your visualization, noting down at least 2 areas or patterns of interest**. These may be a local extrema, or a region that is strangely all similar.\n",
    "\n",
    "**Hint**: Use [`to_crs`](https://geopandas.org/docs/reference/api/geopandas.GeoDataFrame.to_crs.html) and make sure the `epsg` is using the Pseudo-Mercator projection.\n",
    "\n",
    "**Hint**: You can use `contextily` to superimpose your chloropleth map on a real geographic map.\n",
    "\n",
    "**Hint** You can set a lower opacity for your chloropleth map, to see what's underneath, but be aware that if you plot with too low of an opacity, the map underneath will perturb your chloropleth and meddle with your conclusions.\n",
    "\n",
    "Written answers in the first cell, coding answers in the second cell.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2bi1\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd3d7a",
   "metadata": {},
   "source": [
    "The region near the SFO airport increased the most in speed. This makes sense because, after the lockdown, there weren’t many people traveling using airplanes, meaning highways and other roads near SFO became much less congested. Due to this, traffic speeds increased drastically. \n",
    "\n",
    "Another interesting pattern we noticed was that the average speed in downtown San Fransisco did not have a drastic change before and after the lockdown. This seemed quite strange due to the fact that downtown is considered an extremely crowded region, which made me assume that after the lockdown there should be much less people traveling downtown. I would assume that post lockdown, the speed of traffic should have changed drastically. That said, these roads might have other conditions that slow down traffic other than density of traffic. Steep hills, narrow roads, low speed limits, etc., likely kept traffic speeds low even when there wasn’t much congestion post-lockdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0351d12",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2bi2\n",
    "points: 4\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d680081",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1, 1)\n",
    "avgs_pre_heatmap = gpd.GeoDataFrame(averages_pre_named).to_crs(3857) \n",
    "avgs_pre_heatmap.plot('speed_mph_mean', ax = ax, legend = True) \n",
    "cx.add_basemap(ax, crs = avgs_pre_heatmap.crs.to_string(), source = cx.providers.Stamen.TonerLite);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c6a78",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.b.ii. Visualize change in average daily speeds pre vs. post lockdown.\n",
    "\n",
    "Visualize a spatial heatmap of the census tract differences in average speeds, that we computed in a previous part. **Write your observations, using your visualization, noting down at least 2 areas or patterns of interest.** Some possible ideas for interesting notes: Which areas saw the most change in average speed? Which areas weren't affected? Why did some areas see *reduced* average speed?\n",
    "\n",
    "First cell is for the written answers, second cell is for the coding answers.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2bii1\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846910d",
   "metadata": {},
   "source": [
    "One interesting pattern I noticed from the plot was that downtown San Francisco had the lowest average speed, which was close to 20 mph. This makes sense because downtown San Francisco is the most populated place with the most traffic, so the average speed during pre-lockdown should be slower than post-lockdown. \n",
    "\n",
    "Another observation is it looks like the area around Pacifica, the south-west most region on the heatmap, actually saw a reduced average speed after the lockdown. This could be because that area is home to a huge number of hiking trails, beaches, and other outdoor activities that seemingly saw an increase in traffic after the lockdown. This is likely because people were extremely limited in activities, and being outdoors was one of the only ways people could spend their time outside of their home."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e13db6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2bii2\n",
    "points: 4\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00384667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1, 1)\n",
    "speed_change_heatmap = gpd.GeoDataFrame(diff, geometry = 'geometry_x').to_crs(3857)\n",
    "speed_change_heatmap.plot('differences', ax = ax, legend = True) \n",
    "cx.add_basemap(ax, crs = avgs_pre_heatmap.crs.to_string(), source = cx.providers.Stamen.TonerLite);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3ac2e",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Step 3 - Open-Ended EDA: Understanding lockdown impact on travel times\n",
    "\n",
    "Explore daily travel times from Hayes Valley to other destinations both before and throughout lockdown. Use the following questions as suggestions for what to explore, temporally and spatially:\n",
    "\n",
    "- How did lockdown affect travel times? Are there any meaningful factors that determined how travel time would be impacted? How was travel time affected over time?\n",
    "- Travel to which destinations were affected by lockdown? Are there surprisingly disproportionate amounts of impact in certain areas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfca76",
   "metadata": {},
   "source": [
    "## 3.a. Load Datasets\n",
    "\n",
    "In this step, we will load two datasets:\n",
    "\n",
    "- Daily travel times from Hayes Valley to all other census tracts around San Francisco.\n",
    "- Daily travel times from 300 Hayes St to Golden Gate Park in San Francisco.\n",
    "\n",
    "For this specific set of data, we can ask several more questions; which questions you pursue are up to you, including any that you come up that are not on this list:\n",
    "\n",
    "- Which routes from Hayes Valley had similar impact on travel time? Did they share any factors in common? Traveling through the same place -- e.g., a freway? Traveling in similar areas e.g., residential areas?\n",
    "- Were clusters of routes impacted more severely than others over time? What determined the degree of impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8c734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH_TIMES = 'data/travel-times-daily-san-francisco-2020-3.csv'\n",
    "times_to_tract = pd.read_csv(PATH_TIMES)\n",
    "times_to_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecaf18-372f-4007-a64d-6f90cdf3fb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2324d4cc",
   "metadata": {},
   "source": [
    "# Step 4 - Guided Modeling: Predict traffic speed post-lockdown\n",
    "\n",
    "In this step, you'll train a model to predict traffic speed. In particular, you'll learn how to provide implicit supervision and correction to your model, when you know there's been a distribution shift in its data, leading to a large gap between train and test sets. You'll follow the following outline:\n",
    "\n",
    "- Build a model to predict daily traffic speed in San Francisco. Train and evaluate on *pre*-lockdown traffic speeds around the city.\n",
    "- Evaluate your model on post-lockdown traffic speeds. Where is your model most mistaken, and why?\n",
    "- Using this knowledge, how would you correct your model for a more accurate post-lockdown traffic predictor?\n",
    "\n",
    "\n",
    "The technical term for a phenomenon like the lockdown, which caused major distributional shifts in the data, is *change point*. A large body of work studies \"change point detection,\" but you'll be harder pressed to find a \"handling change point\" paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c0fb2",
   "metadata": {},
   "source": [
    "## 4.a. Predict daily traffic speed on pre-lockdown data\n",
    "\n",
    "For your model, you will predict daily traffic speed per census tract, given the previous $k=5$ daily traffic speeds for that census tract. In particular, say a matrix $A$ is $n \\times d$, where $n$ is the number of census tracts and $d$ is the number of days. We define the following inputs and labels:\n",
    "\n",
    "$$X_{(i,t)} = [A_{(i,t-5)}, A_{(i,t-4)}, A_{(i,t-3)}, A_{(i,t-2)}, A_{(i,t-1)}]$$\n",
    "$$y_{(i,t)} = [A_{(i,t)}]$$\n",
    "\n",
    "This just means that each sample $X_i$ includes speed averages from the previous 5 days for the $i$th census track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1a0de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.a.i. Assemble dataset to predict daily traffic speed.\n",
    "\n",
    "Below, we've included skeletons for the helper functions we defined, to complete the problem. We highly recommend following this skeleton code, else we cannot guarantee staff support for debugging your work.\n",
    "\n",
    "\n",
    "**Hint**: What's wrong with collecting all samples, then randomly selecting some percentage to hold out? See the answer in the expandable below.\n",
    "\n",
    "<details>\n",
    "    <summary>[Click to expand] How to do train-validation split correctly, on time series</summary>\n",
    "    \n",
    "For a *time series* in particular, this random split would be cheating, because data within each day is highly correlated. Instead, you should hold out entire days from the dataset. In this case, you should hold out the last 2 days for your validation set.\n",
    "</details>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4ai1\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_time_series(df: pd.DataFrame):\n",
    "    \"\"\"Convert your dataframe into a 'time series'.\n",
    "    \n",
    "    :param df: the original dataframe, mapping speeds to census tracts.\n",
    "        This dataframe should contain the `MOVEMENT_ID` (census tract id),\n",
    "        `day`, and average speed for that day `speed_mph_mean`\n",
    "    :return: a new dataframe that is formatted as n x d, where\n",
    "        n is the number of samples (census tracts) and d is the number of\n",
    "        dimensions (days). The values are the speeds.\n",
    "    \"\"\"\n",
    "    return pd.pivot_table(data = df, columns = 'day', index = 'MOVEMENT_ID', values = 'speed_mph_mean')\n",
    "time_series = dataframe_to_time_series(speeds_to_tract)\n",
    "time_series_pre = time_series.iloc[:, list(range(13))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c38a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ai1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_to_numpy(df: pd.DataFrame, T: int, n_val: int):\n",
    "    \"\"\"Convert your 'time series' into train-validate splits, in numpy\n",
    "    \n",
    "    You can assume your dataframe contains a `day` column where days\n",
    "    start from 1 and are consecutive.\n",
    "    \n",
    "    :param df: the dataframe formatted as n x d, where\n",
    "        n is the number of samples (census tracts) and d is the number of\n",
    "        dimensions (days). The values are the speeds.\n",
    "    :param T: number of days to include in each training sample\n",
    "    :param n_val: number of days to hold out for the validation set.\n",
    "        Say we have 5 total days in our dataset, T=2, n_val=2. This means\n",
    "        during training, we have samples that pull averages from days 1 and\n",
    "        2 to predict day 3: x=(1, 2), y=(3,) For validation, we have samples\n",
    "        like x=(2, 3), y=(4,) and x=(3, 4), y=(5,). This way, the model sees\n",
    "        data from days 4 and 5 only during validation.\n",
    "    :return: Set of 4 numpy arrays - X_train, y_train, X_val, y_val - where\n",
    "        X_* arrays are (n, T) and y_* arrays are (n,).\n",
    "    \"\"\"\n",
    "    N = df.shape[1]\n",
    "    df_train = df.iloc[:, :N - n_val]\n",
    "    df_val = df.iloc[:, N - n_val - T:N]\n",
    "    first = pd.DataFrame()\n",
    "    second = pd.DataFrame()\n",
    "    for item in range(N - n_val - T): \n",
    "        shift = df_train.shift(-item, axis = 1).iloc[:, :T + 1]\n",
    "        first = first.append(shift)\n",
    "    for item in range(n_val): \n",
    "        shift = df_val.shift(-item, axis = 1).iloc[:, :T + 1]\n",
    "        second = second.append(shift)\n",
    "    X_train = np.array((first.iloc[:, :-1]))\n",
    "    y_train = np.array(first.iloc[:, -1:].unstack())\n",
    "    X_val = np.array((second.iloc[:, :-1]))\n",
    "    y_val = np.array(second.iloc[:, -1:].unstack())\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def remove_nans(X: np.array, y: np.array):\n",
    "    \"\"\"Remove all nans from the provided (X, y) pair.\n",
    "    \n",
    "    Note: A nan in X means that sample must be removed from *both X and y.\n",
    "        Likewise, a nan in y means that sample must be removed from *both\n",
    "        X and y.\n",
    "    \n",
    "    :param X: (n, T) array of model inputs\n",
    "    :param y: (n,) array of labels\n",
    "    :return: (X, y)\n",
    "    \"\"\"\n",
    "    if not len(X):\n",
    "        return X, y\n",
    "    df = pd.DataFrame(columns = list(range(X.shape[1])), data = X)\n",
    "    df['y'] = y \n",
    "    df = df.dropna()\n",
    "    X = np.array(df.iloc[:, :-1])\n",
    "    y = np.array(df.iloc[:, -1:].unstack())\n",
    "    return X, y\n",
    "\n",
    "answer = time_series_to_numpy(time_series, 10, 2)\n",
    "answer2 = remove_nans(answer[0], answer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e28e6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ai2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd28eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_to_dataset(time_series: pd.DataFrame, T: int, n_val: int):\n",
    "    \"\"\"Convert 'time series' dataframe to a numpy dataset.\n",
    "    \n",
    "    Uses utilites above `time_series_to_numpy` and `remove_nans`\n",
    "    \n",
    "    For description of arguments, see `time_series_to_numpy` docstring.\n",
    "    \"\"\"\n",
    "    convert = time_series_to_numpy(time_series, T, n_val)\n",
    "    X_train, y_train = remove_nans(convert[0], convert[1])\n",
    "    X_val, y_val = remove_nans(convert[2], convert[3])\n",
    "    return X_train, y_train, X_val, y_val\n",
    "X_train, y_train, X_val, y_val = time_series_to_dataset(time_series_pre, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ff589",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ai3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9183455",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.a.ii. Train and evaluate linear model on pre-lockdown data.\n",
    "\n",
    "1. **Train a linear model that forecasts the next day's speed average** using your training dataset `X_train`, `y_train`. Specifically, predict $y_{(i,t)}$ from $X_{(i,t)}$, where\n",
    "- $y_{(i,t)}$ is the daily speed average for day $t$ and census tract $i$\n",
    "- $X_{(i,t)}$ is a vector of daily speed averages for days $t-5,t-4,t-3,t-2,t-1$ for census tract $i$\n",
    "2. **Evaluate your model** on your validation dataset `X_val`, `y_val`.\n",
    "3. **Make a scatter plot**, plotting predicted averages against ground truth averages. Note the perfect model would line up all points along the line $y=x$.\n",
    "\n",
    "Our model is quantitatively and qualitatively pretty accurate at this point, training and evaluating on pre-lockdown data.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4aii1\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be823d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train) # set to trained linear model\n",
    "score = reg.score(X_val, y_val) # report r^2 score\n",
    "predict = reg.predict(X_val)\n",
    "# create the scatter plot below\n",
    "plt.scatter(predict, y_val)\n",
    "plt.xlabel('Predicted Averages')\n",
    "plt.ylabel('Ground Truth Averages')\n",
    "plt.title('Predicted Averages Against Ground Truth Averages');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904db8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4aii2\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74913fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc1e54",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4aii2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfd2af",
   "metadata": {},
   "source": [
    "## 4.b. Understand failures on post-lockdown data\n",
    "\n",
    "Your dataset is distributed spatially and temporally. As a result, the most intuitive spaces to visualize your model error or performance along is both spatially and temporally. In this step, we focus on understanding *where* your model fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43fe82",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.b.i. Evaluate on post-lockdown data\n",
    "\n",
    "1. Using your previously trained linear regression model `reg`, **evaluate on post-lockdown data**, meaning daily speed averages on March 14, 2020. Evaluate on all census tracts.\n",
    "2. **Make a scatter plot**, plotting predicted averages against ground truth averages. Note the perfect model would line up all points along the line $y=x$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4bi1\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8edff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_x_pre = time_series.iloc[:, 8:13].to_numpy() # get 'time series' dataframe for days 8, 10, 11, 12, 13\n",
    "time_series_y_post = time_series.iloc[:, 13:14].to_numpy() # get 'time series' dataframe for 14th\n",
    "time_series_x_pre, time_series_y_post = remove_nans(time_series_x_pre, time_series_y_post)\n",
    "score_pre_14th = reg.score(time_series_x_pre, time_series_y_post)\n",
    "score_pre_14th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46315050",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4bi1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_x_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278129b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_series_y_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91da701",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Make scatter plot below.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4bi2\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385011bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = reg.predict(time_series_x_pre)\n",
    "# create the scatter plot below\n",
    "plt.scatter(predict, time_series_y_post)\n",
    "plt.xlabel('Predicted Averages')\n",
    "plt.ylabel('Ground Truth Averages')\n",
    "plt.title('Predicted Averages Against Ground Truth Averages');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e05436",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.b.ii. Report model performance temporally\n",
    "\n",
    "1. **Make a line plot** showing performance of the original model throughout all of March 2020.\n",
    "2. **Report the lowest point on the line plot**, reflecting the lowest model performance.\n",
    "2. **Why is model performance the worst on the 17th?** Why does it begin to worsen on march 15th? And continue to worsen? Use what you know about covid measures on those dates. You may find this webpage useful: https://abc7news.com/timeline-of-coronavirus-us-covid-19-bay-area-sf/6047519/\n",
    "3. **Is the dip in performance on the 9th foreshadowed** by any of our EDA?\n",
    "4. **How does the model miraculously recover on its own?**\n",
    "5. **Make a scatter plot**, plotting predicted averages against ground truth averages *for model predictions on March 17th*. Note the perfect model would line up all points along the line $y=x$. When compared against previous plots of this nature, this plot looks substantially worse, with points straying far from $y=x$.\n",
    "\n",
    "**Note:** Answer questions 2-5 in the Markdown cell below. Q1 and Q6 are answered in the two code cells below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4bii1\n",
    "points: 3\n",
    "manual: True\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae80544",
   "metadata": {},
   "source": [
    "2.The lowest point on the line plot seems to be on the 17th day in March with a model score below 0.75.\n",
    "\n",
    "3.Given that we know that covid lockdowns began on March 14th, the model performance begins to worsen before March 15th. This is because our model is predicting 5 days prior to the present day. The models performance is the worst on the 17th because the model we are using to predict uses the traffic speeds of the last 5 days and because the shutdown was on the 14ths, 2 of the speeds included were prior to the shutdown, 2 of them were post shutdown and one of them was the exact day of the shutdown. Therefore, this model was a bad predictor of model performance temporally.\n",
    "\n",
    "4.The dip in performance on the 9th is foreshadowed by our EDA because our prediction is predicting 5 days prior to the actual day. The covid lockdown began on March 14th, and March 9th is 5 days before the actual covid lockdown, which means that our EDA did foreshadow this dip in the model performance.\n",
    "\n",
    "5.The model miraculously recovers on its own because as we get further in the month and the model uses the 5 day prior method, the 5 days before a later day in the month would use only data from post-lockdown which makes it have a better model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2cda7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Generate line plot.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4bii2\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19a9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array = []\n",
    "for i in np.arange(0, 25):\n",
    "    x_train_b = time_series.iloc[:, i:i+5].to_numpy()\n",
    "    y_train_b = time_series.iloc[:, i+5:i+6].to_numpy()\n",
    "    x_train_b, y_train_b = remove_nans(x_train_b, y_train_b)\n",
    "    array = np.append(array, reg.score(x_train_b, y_train_b))\n",
    "plt.plot(np.arange(6, 31), array)\n",
    "plt.xlabel(\"Day in March\")\n",
    "plt.ylabel(\"Model Score\")\n",
    "plt.title(\"Model Score vs. Day in March\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd8a4c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Generate a scatter plot.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4bii3\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = remove_nans(time_series.iloc[:, 11:16].to_numpy(), time_series.iloc[:, 16:17].to_numpy())\n",
    "predict = reg.predict(X)\n",
    "plt.scatter(x = predict, y = y)\n",
    "plt.xlabel(\"Predicted Averages\")\n",
    "plt.ylabel(\"Ground Truth Averages\")\n",
    "plt.title(\"Predicted Averages Against Ground Truth Averages\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad11a5",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## 4.c. \"Fix\" model on post-lockdown data\n",
    "\n",
    "Per this survey https://pure.tue.nl/ws/files/3488790/740215.pdf, there are 4 categories of fixes for change points:\n",
    "- Forgetting mechanisms\n",
    "- Explicit change detection\n",
    "- Ensemble techniques\n",
    "- Context-aware approaches\n",
    "\n",
    "In this part, we'll combine insights in model errors with previous EDA insights to produce a fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219adccd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.c.i. Learn delta off of a moving bias\n",
    "\n",
    "According to our previous work in EDA, the average speed shoots upwards sharply. As a result, our trick to learn delta the around the average and to naively assume that the average of day $t$ is the average for day $t+1$. We will do this in 4 steps:\n",
    "\n",
    "1. **Create a dataset for your delta model**.\n",
    "2. **Train your delta model** on pre-lockdown data.\n",
    "3. **Evaluate your model on pre-lockdown data**, to ensure that the model has learned to a satisfactory degree, in the nominal case. Remember the naive model achieved 0.97 r^2 on pre-lockdown data.\n",
    "4. **Evaluate your model on the 17th**, to compare against the naive model also evaluated on that day. Notice that your r^2 score has improved by 10%+. Why is your delta model so effective for the 17th?\n",
    "5. **Evaluate your model on the 14th**, to compare against the naive model also evaluated on that day. Notice that your r^2 score is now complete garbage. Why is your delta so ineffective for the 14th?\n",
    "\n",
    "**Hint**: As you build your datasets, always check to make sure you're using the right days! It's easy to have a one-off error that throws off your results.\n",
    "\n",
    "Write your written questions in the next cell, then write the code in the following cells.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4ci1\n",
    "points: 2\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dafac4",
   "metadata": {},
   "source": [
    "The delta model is so effective for the 17th because we used the 5 day prior model which has the speeds of pre-lockdown, the actual lockdown date and post-lockdown, which was a great way to predict a given day post-lockdown.\n",
    "\n",
    "The delta model is so ineffective for the 14th because this was the exact day of the lockdown and if we still used the 5 day prior model, the 5 days before the 14ths were all pre-lockdown, so it would not have predicted a massive drop due to the lockdown on that day. In other words, because we used 5 days that were pre-covid lockdown to predict a given date(the given date being the exact day of the shutdown), it would not have predicted this happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d3d68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4ci2\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0171d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_series_delta = speeds_daily[:13]-time_series_pre  # subtract daily average from pre-lockdown 'time series' dataframe `time_series_pre`\n",
    "time_series_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5cf3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ci2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_delta_train, y_delta_train, X_delta_val, y_delta_val = time_series_to_dataset(time_series_delta, 5, 2)\n",
    "reg_delta = LinearRegression().fit(X_delta_train, y_delta_train)\n",
    "res_4ci3 = reg_delta.score(X_delta_val, y_delta_val) # learning delta as easy as learning original dataset!\n",
    "res_4ci3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b32414",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ci3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00733a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = remove_nans(time_series.iloc[:, 11:16].to_numpy(), time_series.iloc[:, 16:17].to_numpy())\n",
    "time_series_x_pre_17 = X - speeds_daily[11:16].array\n",
    "time_series_17 = y - speeds_daily[17]\n",
    "res_4ci4 = reg_delta.score(time_series_x_pre_17, time_series_17)\n",
    "res_4ci4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e617a2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ci4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046257ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = remove_nans(time_series.iloc[:, 8:13].to_numpy(), time_series.iloc[:, 13:14].to_numpy())\n",
    "time_series_x_pre_14 = X - speeds_daily[8:13].array\n",
    "time_series_14 = y - speeds_daily[14]\n",
    "res_4ci5 = reg_delta.score(time_series_x_pre_14, time_series_14)\n",
    "res_4ci5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00819d08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ci5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cce1ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.c.ii. Does it \"solve itself\"? Does the pre-lockdown model predict, after the change point?\n",
    "\n",
    "Had we ignored the problem, would we have been okay? The temporal plot above showing performance over time suggests a partial recovery. **Evaluate the original, naive model on all post-lockdown data** to see. If your final r^2 score does not match the autograder's:\n",
    "- Double check you have selected daily average speeds for the right days, by printing your dataframe.\n",
    "- Double check you're using the right model (a brand new trained model)\n",
    "- Check you're using `T=5, n_val=2`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4cii\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f33751",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_og, y_train_og, X_val_og, y_val_og = time_series_to_dataset(time_series.iloc[:, 13:31], 5, 0)\n",
    "score_og_post = reg.score(X_train_og, y_train_og)\n",
    "score_og_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17be53",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4cii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64c781",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.c.iii. Naively retrain model with post-lockdown data\n",
    "\n",
    "Can we use the same tactics--that we used to train the original model on pre-lockdown data--to train on the post-lockdown data? **Retrain a linear model and evaluate on post-lockdown data only**. You should construct a new dataset using `time_series_to_dataset` using only time series from March 14 to March 31. If your final r^2 score does not match the autograder's:\n",
    "- Double check you have selected daily average speeds for the right days, by printing your dataframe.\n",
    "- Double check you're using the right model (a brand new trained model)\n",
    "- Check you're using `T=5, n_val=2`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4ciii\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00ebaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_post, y_train_post, X_val_post, y_val_post = time_series_to_dataset(time_series.iloc[:, 14:31], 5, 0)\n",
    "score_post = reg.score(X_train_post, y_train_post)\n",
    "score_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb292c1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ciii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17325151",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.c.iv. What if you just ignore the change point?\n",
    "\n",
    "Turns out, this is no good. Even acknowledging the change point and training *either* before *or* after is better. Being ignorant and training on *both* is the worst option, producing a lower r^2.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4civ\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42833f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_iv, y_train_iv, X_val_iv, y_val_iv = time_series_to_dataset(time_series, 5, 5)\n",
    "reg_iv = LinearRegression().fit(X_train_iv, y_train_iv)\n",
    "res_4civ = reg_iv.score(X_val_iv, y_val_iv)\n",
    "res_4civ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c012320",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4civ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc358d",
   "metadata": {},
   "source": [
    "# Step 5 - Open-Ended Modeling: Predicting travel time post-lockdown\n",
    "\n",
    "*This* is the real deal and ultimately what Uber cares about. Traffic speeds is a proxy task, but the bottom line and moneymaking machine relies on this travel time estimation. Focus on designing experiments instead of focusing on experimental, quantitative results. Your experiments are successful if they inform a decision, even despite a lower-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a75132",
   "metadata": {},
   "source": [
    "## Question 5a\n",
    "\n",
    "Train a baseline model of your choice using any supervised learning approach we have studied; you are not limited to a linear model.\n",
    "\n",
    "\n",
    "**Example**\n",
    "\n",
    "Given the data for this question, you could build a model to predict travel time from Cheesecake Factory to UC Berkeley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab3bc1-2f4c-487b-bf36-42185c5082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = [2,3,4,5,6,9,10,11,12,13,16,17,18,19,20,23,24,25,26,27,30,31]\n",
    "weekday = speeds_to_tract[speeds_to_tract['day'].isin(weekdays)]\n",
    "weekday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522c2e5-191f-458e-b1ba-1490545100e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekends = [1, 7, 8, 14, 15, 21, 22, 28, 29]\n",
    "weekend = speeds_to_tract[speeds_to_tract['day'].isin(weekends)]\n",
    "weekend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75776f23-dd7a-499f-8a22-2e40457621b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_averages_pre = weekday[weekday['day'] < 14].groupby('MOVEMENT_ID').agg('mean').sort_values(by ='speed_mph_mean')['speed_mph_mean']\n",
    "weekday_averages_pre_named = weekday[['MOVEMENT_ID', 'DISPLAY_NAME','geometry']].drop_duplicates().merge(weekday_averages_pre.to_frame().reset_index())\n",
    "weekday_averages_pre_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cbaed-6173-4be9-a3a8-9a0f7c1e228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_averages_post = weekday[weekday['day'] >= 14].groupby('MOVEMENT_ID').agg('mean').sort_values(by ='speed_mph_mean')['speed_mph_mean']\n",
    "weekday_averages_post_named = weekday[['MOVEMENT_ID', 'DISPLAY_NAME','geometry']].drop_duplicates().merge(weekday_averages_post.to_frame().reset_index())\n",
    "weekday_averages_post_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b277744-da19-476f-a87a-59493aa2f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weekday_averages_pre)\n",
    "plt.xlabel(\"Average Speed Within Census Tract (mph)\")\n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Bay Area Census Tracts Average Speed Pre Lockdown on Weekdays\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca3f7d-af56-4a92-aca0-a7ee2c5559e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weekday_averages_post)\n",
    "plt.xlabel(\"Average Speed Within Census Tract (mph)\")\n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Bay Area Census Tracts Average Speed Post Lockdown on Weekdays\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6bf41-9e72-4d1a-90c5-6644174f7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_weekday = weekday_averages_pre_named.merge(weekday_averages_post_named, left_on = \"DISPLAY_NAME\",right_on = \"DISPLAY_NAME\", how = 'inner')\n",
    "diff_weekday['differences'] = diff_weekday['speed_mph_mean_y'] - diff_weekday['speed_mph_mean_x']\n",
    "differences_weekday = diff_weekday['differences']\n",
    "# plot the differences\n",
    "plt.hist(differences_weekday);\n",
    "plt.xlabel(\"Difference in Average Speed between Pre-Lockdown and Post-Lockdown\") \n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Differences in Average Speeds Before and After Lockdown on Weekdays\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8afbcc-ea44-4f0d-98a7-ec138f226cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_averages_pre = weekend[weekend['day'] < 14].groupby('MOVEMENT_ID').agg('mean').sort_values(by ='speed_mph_mean')['speed_mph_mean']\n",
    "weekend_averages_pre_named = weekend[['MOVEMENT_ID', 'DISPLAY_NAME','geometry']].drop_duplicates().merge(weekend_averages_pre.to_frame().reset_index())\n",
    "weekend_averages_pre_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5d282-de01-46df-bcdf-cd06d5ce88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_averages_post = weekend[weekend['day'] >= 14].groupby('MOVEMENT_ID').agg('mean').sort_values(by ='speed_mph_mean')['speed_mph_mean']\n",
    "weekend_averages_post_named = weekend[['MOVEMENT_ID', 'DISPLAY_NAME','geometry']].drop_duplicates().merge(weekend_averages_post.to_frame().reset_index())\n",
    "weekend_averages_post_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14fdfb-329c-4522-bde0-300adf2785cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weekend_averages_pre)\n",
    "plt.xlabel(\"Average Speed Within Census Tract (mph)\")\n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Bay Area Census Tracts Average Speed Post Lockdown on Weekends\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda94452-ce69-4ddd-9a4f-b9b42c3e82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weekend_averages_post)\n",
    "plt.xlabel(\"Average Speed Within Census Tract (mph)\")\n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Bay Area Census Tracts Average Speed Post Lockdown on Weekends\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61791c-563c-402f-9a5b-917e3fc50cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_weekend = weekend_averages_pre_named.merge(weekend_averages_post_named, left_on = \"DISPLAY_NAME\",right_on = \"DISPLAY_NAME\", how = 'inner')\n",
    "diff_weekend['differences'] = diff_weekend['speed_mph_mean_y'] - diff_weekend['speed_mph_mean_x']\n",
    "differences_weekend = diff_weekend['differences']\n",
    "# plot the differences\n",
    "plt.hist(differences_weekend);\n",
    "plt.xlabel(\"Difference in Average Speed between Pre-Lockdown and Post-Lockdown\") \n",
    "plt.ylabel(\"Number of Census Tracts\")\n",
    "plt.title(\"Differences in Average Speeds Before and After Lockdown on Weekends\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae27388-7b80-4318-b34a-b8d4d1eb842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TIMES = 'data/travel-times-daily-san-francisco-2020-3.csv'\n",
    "\n",
    "hayes_valley = pd.read_csv(PATH_TIMES)\n",
    "\n",
    "hayes_valley = hayes_valley.drop(columns = [\"Origin Movement ID\", \"Origin Display Name\", \"Date Range\"])\n",
    "\n",
    "hayes_valley[\"Average Bound Difference\"] = hayes_valley[\"Range - Upper Bound Travel Time (Seconds)\"] - hayes_valley[\"Range - Lower Bound Travel Time (Seconds)\"] \n",
    "\n",
    "hayes_valley.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d6376-795a-448b-8ea1-5e5f395ebec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hayes_valley_pre = hayes_valley[hayes_valley[\"day\"] < 14]\n",
    "hayes_valley_post = hayes_valley[hayes_valley[\"day\"] >= 14]\n",
    "\n",
    "hayes_valley_pre_grouped = hayes_valley_pre.groupby(\"Destination Movement ID\").agg('mean')\n",
    "hayes_valley_post_grouped = hayes_valley_post.groupby(\"Destination Movement ID\").agg('mean')\n",
    "\n",
    "hayes_valley_pre_grouped = hayes_valley_pre_grouped.drop(columns = [\"Range - Lower Bound Travel Time (Seconds)\", \"Range - Upper Bound Travel Time (Seconds)\", \"day\"]).sort_values(by=['Destination Movement ID'])\n",
    "hayes_valley_post_grouped = hayes_valley_post_grouped.drop(columns = [\"Range - Lower Bound Travel Time (Seconds)\", \"Range - Upper Bound Travel Time (Seconds)\", \"day\"]).sort_values(by=['Destination Movement ID'])\n",
    "hayes_valley_post_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7325673-e1a3-4b8f-b311-43c77236a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_removed = hayes_valley_pre_grouped.copy() \n",
    "post_removed = hayes_valley_post_grouped.copy() \n",
    "\n",
    "pre_tracts = hayes_valley_pre_grouped.index\n",
    "pre_tracts\n",
    "\n",
    "post_tracts = hayes_valley_post_grouped.index\n",
    "post_tracts\n",
    "\n",
    "remove_these_pre = []\n",
    "remove_these_post = []\n",
    "\n",
    "for i in pre_tracts:\n",
    "    if i not in post_tracts:\n",
    "        remove_these_pre.append(i)\n",
    "        \n",
    "for i in post_tracts:\n",
    "    if i not in pre_tracts:\n",
    "        remove_these_post.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bddebe-3f75-4863-a4f0-3fe97379f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in remove_these_pre:\n",
    "    pre_removed = pre_removed.drop(i)\n",
    "    \n",
    "for i in remove_these_post:\n",
    "    post_removed = post_removed.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901c7e8-e6e4-4be0-a14b-da214beb083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "final = pd.DataFrame()\n",
    "final.index = pre_removed.index\n",
    "final[\"Pre Lockdown Average Bound Size\"] = pre_removed[\"Average Bound Difference\"]\n",
    "final[\"Post Lockdown Average Bound Size\"] = post_removed[\"Average Bound Difference\"]\n",
    "final[\"Pre Lockdown Average Travel Time\"] = pre_removed[\"Mean Travel Time (Seconds)\"]\n",
    "final[\"Post Lockdown Average Travel Time\"] = post_removed[\"Mean Travel Time (Seconds)\"]\n",
    "final[\"Increase in Average Bound Size Post Lockdown\"] = final[\"Post Lockdown Average Bound Size\"] - final[\"Pre Lockdown Average Bound Size\"]\n",
    "\n",
    "corr_bound_diff_pre = np.corrcoef(final[\"Increase in Average Bound Size Post Lockdown\"], final[\"Pre Lockdown Average Bound Size\"])[1][0]\n",
    "corr_bound_diff_post = np.corrcoef(final[\"Increase in Average Bound Size Post Lockdown\"], final[\"Post Lockdown Average Bound Size\"])[1][0]\n",
    "\n",
    "final.plot.scatter(x = \"Increase in Average Bound Size Post Lockdown\", y = \"Post Lockdown Average Bound Size\", title = \"Increase in Bound Size vs. Average Bound Size (Post Lockdown)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be356301-caab-416d-98df-3deb3145fd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32ba1e-f1de-4f74-9ed8-70efdc711bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_weekdays = time_series.loc[:, weekdays]\n",
    "time_series_weekends = time_series.loc[:, weekends]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc6e1e-5393-431f-a6ef-033b95f30f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekday model to predict weekday traffic speeds\n",
    "X_train_w, y_train_w, X_val_w, y_val_w = time_series_to_dataset(time_series_weekdays, 5, 2)\n",
    "reg_w = LinearRegression().fit(X_train_w, y_train_w) # set to trained linear model\n",
    "score_w = reg_w.score(X_val_w, y_val_w) \n",
    "predict_w = reg_w.predict(X_val_w)\n",
    "# create the scatter plot below\n",
    "plt.scatter(predict_w, y_val_w)\n",
    "plt.xlabel('Predicted Averages')\n",
    "plt.ylabel('Ground Truth Averages')\n",
    "plt.title('Weekday Model Evaluated on Weekday Data: Predicted Averages Against Ground Truth Averages');\n",
    "score_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf0824-8a1d-4370-a5b9-14d755d8b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekend model to predict weekend traffic speeds\n",
    "X_train_end, y_train_end, X_val_end, y_val_end = time_series_to_dataset(time_series_weekends, 5, 2)\n",
    "reg_end = LinearRegression().fit(X_train_end, y_train_end) \n",
    "score_end = reg_end.score(X_val_end, y_val_end) \n",
    "predict_end = reg_end.predict(X_val_end)\n",
    "# create the scatter plot below\n",
    "plt.scatter(predict_end, y_val_end)\n",
    "plt.xlabel('Predicted Averages')\n",
    "plt.ylabel('Ground Truth Averages')\n",
    "plt.title('Weekend Model Evaluated on Weekend Data: Predicted Averages Against Ground Truth Averages');\n",
    "score_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead1d5e-1c58-41b8-b50f-448662de826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekday model to predict weekend traffic speeds\n",
    "X_train_end, y_train_end, X_val_end, y_val_end = time_series_to_dataset(time_series_weekends, 5, 2)\n",
    "score_week = reg_w.score(X_val_end, y_val_end) \n",
    "predict_week = reg_w.predict(X_val_end)\n",
    "# create the scatter plot below\n",
    "plt.scatter(predict_week, y_val_end)\n",
    "plt.xlabel('Predicted Averages')\n",
    "plt.ylabel('Ground Truth Averages')\n",
    "plt.title('Weekday Model Evaluated on Weekend Data: Predicted Averages Against Ground Truth Averages');\n",
    "score_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededc6c1-9162-4ab6-9dd6-556253dcff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekend model to predict weekday traffic speeds\n",
    "X_train_w, y_train_w, X_val_w, y_val_w = time_series_to_dataset(time_series_weekdays, 5, 2)\n",
    "score_weeks = reg_end.score(X_val_w, y_val_w) \n",
    "predict_weeks = reg_end.predict(X_val_w)\n",
    "# create the scatter plot below\n",
    "plt.scatter(predict_weeks, y_val_w)\n",
    "plt.xlabel('Predicted Averages')\n",
    "plt.ylabel('Ground Truth Averages')\n",
    "plt.title('Weekend Model Evaluated on Weekday Data: Predicted Averages Against Ground Truth Averages');\n",
    "score_weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3fdb3",
   "metadata": {},
   "source": [
    "## Question 5b\n",
    "\n",
    "Improve on your baseline model. Specify the model you designed and its input features. Justify why you chose these features and their relevance to your model's predictions.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Here are potential questions to consider for this part: How does the other variant of your travel times dataset, aggregated across time but reported for all routes, useful?  What additional data from the Uber Movement website can you export to better your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf88fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd8e5c3e",
   "metadata": {},
   "source": [
    "## Question 5c\n",
    "\n",
    "Explore other modeling aspects and/or temporal information. You are free to relate this to your hypothesis or not. Please expand into multiple parts that logically separate and break down your modeling work!\n",
    "\n",
    "**Example**\n",
    "\n",
    "For example, explore change across time, before and after the lockdown: (a) train and evaluate on *pre*-lockdown traffic travel times for that route; and (b) evaluate your model on *post*-lockdown traffic patterns.\n",
    "How would you correct your model for a more accurate post-lockdown traffic predictor? *The above is just a suggestion. You may pick any topic you find interesting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648a468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "400dba85",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d058c6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a248128",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e244f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732614ed",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
